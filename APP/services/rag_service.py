"""
RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤

ì—­í• :
- ë¬¸ì„œ ì²­í‚¹ (SmartChunker ì‚¬ìš©)
- ë²¡í„° ìž„ë² ë”© ìƒì„± ë° ì €ìž¥
- ìœ ì‚¬ë„ ê²€ìƒ‰
- ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±
"""
import logging
import re  # âœ… [í•„ìˆ˜] ì •ê·œí‘œí˜„ì‹(ê°•ë ¥ ì„¸íƒìš©)
from typing import Dict, List, Optional
from dataclasses import dataclass, field
import numpy as np

from APP.services.llm_service import (
    generate_embeddings,
    generate_embedding,
    chat_with_context,
    is_available
)
from APP.services.chunker import SmartChunker, ChunkingConfig, Chunk

logger = logging.getLogger(__name__)


# ============================================
# ë°ì´í„° í´ëž˜ìŠ¤
# ============================================

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼"""
    chunk: Chunk
    score: float


# ============================================
# RAG ì‹œìŠ¤í…œ í´ëž˜ìŠ¤
# ============================================

class RAGSystem:
    def __init__(
        self,
        chunk_size: int = 1500,  # âœ… [ìˆ˜ì •] 1íŽ˜ì´ì§€ë¥¼ í†µì§¸ë¡œ ì¸ì‹í•˜ë„ë¡ í¬ê¸° ì¦ê°€
        chunk_overlap: int = 300, # âœ… [ìˆ˜ì •] ë¬¸ë§¥ ëŠê¹€ ë°©ì§€
        top_k: int = 3
    ):
        self.top_k = top_k
        
        # SmartChunker ì´ˆê¸°í™”
        self.chunker = SmartChunker(ChunkingConfig(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            preserve_tables=True,
            preserve_titles=True,
            sentence_boundary=True
        ))
        
        self._storage: Dict[str, Dict] = {}
        logger.info(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”: chunk_size={chunk_size}, overlap={chunk_overlap}")
    
    # ============================================
    # ë¬¸ì„œ ì²˜ë¦¬
    # ============================================
    
    def add_document(self, document_id: str, text: str, metadata: Dict = None) -> int:
        if not is_available():
            raise ValueError("LLM ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # âœ… [í•µì‹¬ ìˆ˜ì •] í…ìŠ¤íŠ¸ ê°•ë ¥ ì„¸íƒ (ì¤„ë°”ê¿ˆ, ì´ìƒí•œ ê³µë°± ì‹¹ ì •ë¦¬)
        if text:
            # 1. íˆ¬ëª… íŠ¹ìˆ˜ë¬¸ìž ì œê±°
            text = text.replace("\u200b", "").replace("\xa0", " ")
            # 2. ê³¼ë„í•œ ì¤„ë°”ê¿ˆ/ê³µë°±ì„ ê³µë°± í•˜ë‚˜ë¡œ í†µì¼ (PDF ì¸ì‹ë¥  200% ìƒìŠ¹ ë¹„ë²•)
            text = re.sub(r'\s+', ' ', text).strip()
            
            print(f"ðŸ§¹ [DEBUG] í…ìŠ¤íŠ¸ ê°•ë ¥ ì„¸íƒ ì™„ë£Œ: {text[:100]}...")  # ë¡œê·¸ë¡œ í™•ì¸

        chunks = self.chunker.chunk(text, document_id)
        
        if not chunks:
            logger.warning(f"âš ï¸ ë¬¸ì„œ {document_id}: ì²­í¬ ìƒì„± ì‹¤íŒ¨")
            return 0
        
        if metadata:
            for chunk in chunks:
                chunk.metadata.update(metadata)
        
        chunk_texts = [c.text for c in chunks]
        embeddings = generate_embeddings(chunk_texts, task_type="retrieval_document")
        
        if embeddings is None:
            logger.error(f"âŒ ë¬¸ì„œ {document_id}: ìž„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            return 0
        
        self._storage[document_id] = {
            "chunks": chunks,
            "embeddings": np.array(embeddings)
        }
        
        logger.info(f"âœ… ë¬¸ì„œ {document_id}: {len(chunks)}ê°œ ì²­í¬ ì €ìž¥ ì™„ë£Œ")
        return len(chunks)
    
    def remove_document(self, document_id: str) -> bool:
        if document_id in self._storage:
            del self._storage[document_id]
            return True
        return False
    
    def has_document(self, document_id: str) -> bool:
        return document_id in self._storage

    # ============================================
    # ê²€ìƒ‰ ë° ì§ˆì˜
    # ============================================

    def search(self, document_id: str, query: str, top_k: int = None) -> List[SearchResult]:
        if document_id not in self._storage:
            return []
        
        top_k = top_k or self.top_k
        query_embedding = generate_embedding(query, task_type="retrieval_query")
        if query_embedding is None:
            return []
        
        query_vec = np.array(query_embedding)
        doc_data = self._storage[document_id]
        embeddings = doc_data["embeddings"]
        chunks = doc_data["chunks"]
        
        similarities = self._cosine_similarity(query_vec, embeddings)
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append(SearchResult(
                chunk=chunks[idx],
                score=float(similarities[idx])
            ))
        return results

    def query(
        self,
        document_id: str,
        question: str,
        history: List[Dict] = None
    ) -> Dict:
        # 1. ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
        results = self.search(document_id, question)
        
        if not results:
            return {
                "answer": "ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "sources": [],
                "confidence": 0.0
            }
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([r.chunk.text for r in results])
        
        # âœ… [ë””ë²„ê¹… ì¶”ê°€] AIê°€ ì‹¤ì œë¡œ ì½ëŠ” ë‚´ìš© ëˆˆìœ¼ë¡œ í™•ì¸í•˜ê¸°
        print("="*50)
        print(f"ðŸ§ [DEBUG] AIì—ê²Œ ë“¤ì–´ê°€ëŠ” ì»¨í…ìŠ¤íŠ¸ ë‚´ìš©:\n{context[:500]}...") # ë„ˆë¬´ ê¸¸ë©´ ìž˜ë¼ì„œ ë³´ì—¬ì¤Œ
        print("="*50)
        
        # 3. ë‹µë³€ ìƒì„±
        answer = chat_with_context(question, context, history)
        
        # 4. ì‹ ë¢°ë„ ê³„ì‚°
        avg_score = sum(r.score for r in results) / len(results)
        
        return {
            "answer": answer,
            "sources": [
                {
                    "text": r.chunk.text[:200],
                    "score": r.score,
                    "chunk_type": r.chunk.chunk_type.value,
                    "index": r.chunk.index
                }
                for r in results
            ],
            "confidence": round(avg_score, 2)
        }
    
    def _cosine_similarity(self, query_vec: np.ndarray, doc_vecs: np.ndarray) -> np.ndarray:
        query_norm = query_vec / np.linalg.norm(query_vec)
        doc_norms = doc_vecs / np.linalg.norm(doc_vecs, axis=1, keepdims=True)
        return np.dot(doc_norms, query_norm)
    
    def get_stats(self) -> Dict:
        total_chunks = sum(len(data["chunks"]) for data in self._storage.values())
        return {
            "document_count": len(self._storage),
            "total_chunks": total_chunks,
            "documents": list(self._storage.keys()),
            "chunker_config": {
                "chunk_size": self.chunker.config.chunk_size,
                "chunk_overlap": self.chunker.config.chunk_overlap,
                "preserve_tables": self.chunker.config.preserve_tables
            }
        }

# ============================================
# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
# ============================================

_rag_instance: Optional[RAGSystem] = None

def get_rag_system() -> RAGSystem:
    global _rag_instance
    if _rag_instance is None:
        _rag_instance = RAGSystem()
    return _rag_instance

def add_document(document_id: str, text: str) -> int:
    return get_rag_system().add_document(document_id, text)

def query_document(document_id: str, question: str) -> Dict:
    return get_rag_system().query(document_id, question)